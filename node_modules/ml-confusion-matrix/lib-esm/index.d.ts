/**
 * Constructs a confusion matrix
 * @class ConfusionMatrix
 * @example
 * const CM = new ConfusionMatrix([[13, 2], [10, 5]], ['cat', 'dog'])
 * @param matrix - The confusion matrix, a 2D Array. Rows represent the actual label and columns the predicted label.
 * @param labels - Labels of the confusion matrix, a 1D Array
 */
export declare class ConfusionMatrix<T extends Label> {
    private labels;
    private matrix;
    constructor(matrix: number[][], labels: T[]);
    /**
     * Construct confusion matrix from the predicted and actual labels (classes). Be sure to provide the arguments in
     * the correct order!
     * @param actual  - The predicted labels of the classification
     * @param predicted - The actual labels of the classification. Has to be of same length as predicted.
     * @param [options] - Additional options
     * @param [options.labels] - The list of labels that should be used. If not provided the distinct set
     *     of labels present in predicted and actual is used. Labels are compared using the strict equality operator
     *     '==='
     * @param [options.sort]
     * @return Confusion matrix
     */
    static fromLabels<T extends Label>(actual: T[], predicted: T[], options?: FromLabelsOptions<T>): ConfusionMatrix<T>;
    /**
     * Get the confusion matrix
     */
    getMatrix(): number[][];
    getLabels(): T[];
    /**
     * Get the total number of samples
     */
    getTotalCount(): number;
    /**
     * Get the total number of true predictions
     */
    getTrueCount(): number;
    /**
     * Get the total number of false predictions.
     */
    getFalseCount(): number;
    /**
     * Get the number of true positive predictions.
     * @param label - The label that should be considered "positive"
     */
    getTruePositiveCount(label: T): number;
    /**
     * Get the number of true negative predictions.
     * @param label - The label that should be considered "positive"
     */
    getTrueNegativeCount(label: T): number;
    /**
     * Get the number of false positive predictions.
     * @param label - The label that should be considered "positive"
     */
    getFalsePositiveCount(label: T): number;
    /**
     * Get the number of false negative predictions.
     * @param label - The label that should be considered "positive"
     */
    getFalseNegativeCount(label: T): number;
    /**
     * Get the number of real positive samples.
     * @param label - The label that should be considered "positive"
     */
    getPositiveCount(label: T): number;
    /**
     * Get the number of real negative samples.
     * @param  label - The label that should be considered "positive"
     */
    getNegativeCount(label: T): number;
    /**
     * Get the index in the confusion matrix that corresponds to the given label
     * @param label - The label to search for
     * @throws if the label is not found
     */
    getIndex(label: T): number;
    /**
     * Get the true positive rate a.k.a. sensitivity. Computes the ratio between the number of true positive predictions and the total number of positive samples.
     * {@link https://en.wikipedia.org/wiki/Sensitivity_and_specificity}
     * @param label - The label that should be considered "positive"
     * @return The true positive rate [0-1]
     */
    getTruePositiveRate(label: T): number;
    /**
     * Get the true negative rate a.k.a. specificity. Computes the ration between the number of true negative predictions and the total number of negative samples.
     * {@link https://en.wikipedia.org/wiki/Sensitivity_and_specificity}
     * @param label - The label that should be considered "positive"
     * @return The true negative rate a.k.a. specificity.
     */
    getTrueNegativeRate(label: T): number;
    /**
     * Get the positive predictive value a.k.a. precision. Computes TP / (TP + FP)
     * {@link https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values}
     * @param label - The label that should be considered "positive"
     * @return the positive predictive value a.k.a. precision.
     */
    getPositivePredictiveValue(label: T): number;
    /**
     * Negative predictive value
     * {@link https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values}
     * @param label - The label that should be considered "positive"
     */
    getNegativePredictiveValue(label: T): number;
    /**
     * False negative rate a.k.a. miss rate.
     * {@link https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#False_positive_and_false_negative_rates}
     * @param label - The label that should be considered "positive"
     */
    getFalseNegativeRate(label: T): number;
    /**
     * False positive rate a.k.a. fall-out rate.
     * {@link https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#False_positive_and_false_negative_rates}
     * @param  label - The label that should be considered "positive"
     */
    getFalsePositiveRate(label: T): number;
    /**
     * False discovery rate (FDR)
     * {@link https://en.wikipedia.org/wiki/False_discovery_rate}
     * @param label - The label that should be considered "positive"
     */
    getFalseDiscoveryRate(label: T): number;
    /**
     * False omission rate (FOR)
     * @param label - The label that should be considered "positive"
     */
    getFalseOmissionRate(label: T): number;
    /**
     * F1 score
     * {@link https://en.wikipedia.org/wiki/F1_score}
     * @param label - The label that should be considered "positive"
     */
    getF1Score(label: T): number;
    /**
     * Matthews correlation coefficient (MCC)
     * {@link https://en.wikipedia.org/wiki/Matthews_correlation_coefficient}
     * @param label - The label that should be considered "positive"
     */
    getMatthewsCorrelationCoefficient(label: T): number;
    /**
     * Informedness
     * {@link https://en.wikipedia.org/wiki/Youden%27s_J_statistic}
     * @param label - The label that should be considered "positive"
     */
    getInformedness(label: T): number;
    /**
     * Markedness
     * @param label - The label that should be considered "positive"
     */
    getMarkedness(label: T): number;
    /**
     * Get the confusion table.
     * @param label - The label that should be considered "positive"
     * @return The 2x2 confusion table. [[TP, FN], [FP, TN]]
     */
    getConfusionTable(label: T): number[][];
    /**
     * Get total accuracy.
     * The ratio between the number of true predictions and total number of classifications ([0-1])
     */
    getAccuracy(): number;
    /**
     * Returns the element in the confusion matrix that corresponds to the given actual and predicted labels.
     * @param actual - The true label
     * @param predicted - The predicted label
     * @return The element in the confusion matrix
     */
    getCount(actual: T, predicted: T): number;
    /**
     * Compute the general prediction accuracy
     * @deprecated Use getAccuracy
     * @return The prediction accuracy ([0-1]
     */
    get accuracy(): number;
    /**
     * Compute the number of predicted observations
     * @deprecated Use getTotalCount
     */
    get total(): number;
}
type Label = boolean | number | string;
interface FromLabelsOptions<T extends Label> {
    labels?: T[];
    sort?: (...args: Label[]) => number;
}
export {};
//# sourceMappingURL=index.d.ts.map